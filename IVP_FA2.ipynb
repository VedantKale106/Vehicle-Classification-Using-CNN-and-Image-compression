{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt41ItGhvPYZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import heapq\n",
        "import sys\n",
        "from collections import defaultdict\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "train_data = ImageFolder(\"/content/drive/MyDrive/dataset/train\", transform=transform)\n",
        "val_data = ImageFolder(\"/content/drive/MyDrive/dataset/val\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
        "\n",
        "classes = train_data.classes\n"
      ],
      "metadata": {
        "id": "xqw40y8Nxrt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 4)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "a2Uyc7Nixt4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e6a155-9851-4674-b916-2bba0f381363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "2YuLBdr3xzMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d7348b-0430-47dc-bf22-74844b5920af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.9939\n",
            "Epoch [2/10], Loss: 0.5389\n",
            "Epoch [3/10], Loss: 0.3539\n",
            "Epoch [4/10], Loss: 0.3058\n",
            "Epoch [5/10], Loss: 0.2582\n",
            "Epoch [6/10], Loss: 0.2410\n",
            "Epoch [7/10], Loss: 0.2441\n",
            "Epoch [8/10], Loss: 0.1992\n",
            "Epoch [9/10], Loss: 0.1801\n",
            "Epoch [10/10], Loss: 0.1720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RLE\n",
        "def rle_encode(img):\n",
        "    pixels = img.flatten()\n",
        "    encoding = []\n",
        "    prev_pixel = pixels[0]\n",
        "    count = 1\n",
        "    for pixel in pixels[1:]:\n",
        "        if pixel == prev_pixel:\n",
        "            count += 1\n",
        "        else:\n",
        "            encoding.append((prev_pixel, count))\n",
        "            prev_pixel = pixel\n",
        "            count = 1\n",
        "    encoding.append((prev_pixel, count))\n",
        "    return encoding\n",
        "\n",
        "# Huffman\n",
        "class Node:\n",
        "    def __init__(self, freq, symbol, left=None, right=None):\n",
        "        self.freq = freq\n",
        "        self.symbol = symbol\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "    def __lt__(self, nxt):\n",
        "        return self.freq < nxt.freq\n",
        "\n",
        "def huffman_encoding(img):\n",
        "    pixels = img.flatten()\n",
        "    freq = defaultdict(int)\n",
        "    for val in pixels:\n",
        "        freq[val] += 1\n",
        "\n",
        "    heap = [Node(freq[v], v) for v in freq]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        n1 = heapq.heappop(heap)\n",
        "        n2 = heapq.heappop(heap)\n",
        "        merged = Node(n1.freq + n2.freq, None, n1, n2)\n",
        "        heapq.heappush(heap, merged)\n",
        "\n",
        "    root = heap[0]\n",
        "    huff_code = {}\n",
        "\n",
        "    def assign_codes(node, code=''):\n",
        "        if node:\n",
        "            if node.symbol is not None:\n",
        "                huff_code[node.symbol] = code\n",
        "            assign_codes(node.left, code + '0')\n",
        "            assign_codes(node.right, code + '1')\n",
        "\n",
        "    assign_codes(root)\n",
        "    encoded = ''.join([huff_code[p] for p in pixels])\n",
        "    return encoded, huff_code\n",
        "\n",
        "# DCT\n",
        "def apply_dct(img):\n",
        "    img = np.float32(img) / 255.0\n",
        "    dct = cv2.dct(img)\n",
        "    return dct\n",
        "\n",
        "def inverse_dct(dct_img):\n",
        "    return cv2.idct(dct_img)\n",
        "\n",
        "def get_size(obj):\n",
        "    return sys.getsizeof(obj)\n"
      ],
      "metadata": {
        "id": "LRZzs_i-x-kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for images, labels in val_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Apply compression on first image in the batch\n",
        "    img = images[0].cpu().permute(1, 2, 0).numpy()\n",
        "    img = ((img * 0.5) + 0.5) * 255  # Denormalize\n",
        "    img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "\n",
        "\n",
        "print(f\"Validation Accuracy: {(100 * correct / total):.2f}%\")\n"
      ],
      "metadata": {
        "id": "8GpOOeq6x0f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5d2a539-4b87-4b15-d3a9-11f5e844754d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 93.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Preprocessing for model\n",
        "transform_single = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "def predict_from_url(img_url, model):\n",
        "    # Step 1: Download image from URL\n",
        "    response = requests.get(img_url)\n",
        "    img_pil = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "    # Step 2: Convert to grayscale for compression\n",
        "    img_gray = img_pil.convert(\"L\")\n",
        "    img_gray = img_gray.resize((224, 224))\n",
        "    img_np = np.array(img_gray)\n",
        "\n",
        "    # Step 3: Compression\n",
        "    original_size = img_np.size\n",
        "\n",
        "    rle_encoded = rle_encode(img_np)\n",
        "    rle_size = get_size(rle_encoded)\n",
        "\n",
        "    huff_encoded, _ = huffman_encoding(img_np)\n",
        "    huff_size = len(huff_encoded) // 8\n",
        "\n",
        "    # DCT\n",
        "    dct_img = apply_dct(img_np)\n",
        "    recon_img = inverse_dct(dct_img)\n",
        "    recon_img = np.clip(recon_img * 255, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # Step 4: Convert DCT output to RGB format for model\n",
        "    recon_rgb = cv2.cvtColor(recon_img, cv2.COLOR_GRAY2RGB)\n",
        "    recon_pil = Image.fromarray(recon_rgb)\n",
        "\n",
        "    input_tensor = transform_single(recon_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    # Step 5: Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "\n",
        "    print(\"📦 Compression Stats:\")\n",
        "    print(f\"Original size: {original_size}\")\n",
        "    print(f\"RLE size: {rle_size}\")\n",
        "    print(f\"Huffman size: {huff_size}\")\n",
        "    print(\"📸 Reconstructed image (from DCT) sent to model.\")\n",
        "\n",
        "    return classes[predicted.item()]\n"
      ],
      "metadata": {
        "id": "oQMFv0X0x6nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://th.bing.com/th/id/OIP.OmszxJcT8NO06xdukAihmwHaE7?w=266&h=180&c=7&r=0&o=5&pid=1.7\"\n",
        "predicted_class = predict_from_url(img_url, model)\n",
        "print(f\"🚗 Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "p0RkIRqIyQbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1443f08-7ab7-4683-882d-48486fd87f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Compression Stats:\n",
            "Original size: 50176\n",
            "RLE size: 351064\n",
            "Huffman size: 49131\n",
            "📸 Reconstructed image (from DCT) sent to model.\n",
            "🚗 Predicted class: Car\n"
          ]
        }
      ]
    }
  ]
}